{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando as amostras em Treino, Validação e Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook tem como objetivo dividir as amostras em treino, validação e teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao invés de usar uma ferramenta pronta (como a função _train_test_split_ do *sklearn*), mostro aqui o passo a passo para balancear o dataset e fazer a divisão em treino, validação e teste.\n",
    "\n",
    "Uma boa prática que também adotei aqui é verificar se existem amostras repetidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir, mkdir\n",
    "from os.path import isfile, join, isdir\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_files = sorted([join('../data/', f) for f in listdir('../data/') if isfile(join('../data/', f)) and 'trainX' in f])\n",
    "Y_files = sorted([join('../data/', f) for f in listdir('../data/') if isfile(join('../data/', f)) and 'trainY' in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupando todas as amostras em uma única lista\n",
    "all_samples, all_labels = None, None\n",
    "# Percorre par de arquivos\n",
    "for fx,fy in zip(X_files,Y_files):\n",
    "    sample_x = np.load(fx)\n",
    "    label = np.load(fy)\n",
    "    if all_samples is None:\n",
    "        all_samples = sample_x\n",
    "        all_labels = label\n",
    "    else:\n",
    "        all_samples = np.concatenate((all_samples, sample_x))\n",
    "        all_labels = np.concatenate((all_labels, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r# Embaralhando amostras para evitar viés dos dados fornecidos\n",
    "random_seed = 123\n",
    "np.random.seed(random_seed)\n",
    "indexes = np.arange(all_samples.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "all_samples = all_samples[indexes]\n",
    "all_labels = all_labels[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte em listas\n",
    "all_samples = list(all_samples)\n",
    "all_labels = list(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amostras duplicadas removidas: [4380, 3410, 1710, 1113]\n"
     ]
    }
   ],
   "source": [
    "# Verifica possíveis amostras repetidas com checksum\n",
    "checksum = {}\n",
    "for i, sample in enumerate(all_samples):\n",
    "    _sum = sample.sum()\n",
    "    checksum.setdefault(_sum, [])\n",
    "    checksum[_sum].append(i)\n",
    "# Para cada possível amostra repetida, verifica se está realmente duplicada\n",
    "idx_to_remove = []\n",
    "for k, v in checksum.items():\n",
    "    if len(v) == 1:\n",
    "        continue\n",
    "    # Duplicidade confirmada\n",
    "    if (all_samples[v[0]] == all_samples[v[1]]).all():\n",
    "        idx_to_remove.append(v[0])\n",
    "# Inverte indexes para não remover amostras erradas com o pop\n",
    "idx_to_remove.sort(reverse=True)\n",
    "[all_samples.pop(idx) for idx in idx_to_remove]\n",
    "[all_labels.pop(idx) for idx in idx_to_remove]\n",
    "print(f'Amostras duplicadas removidas: {idx_to_remove}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2224, 4444)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide em amostras positivas e negativas\n",
    "all_pos_samples = [sample for i, sample in enumerate(all_samples) if all_labels[i][0] == 0.]\n",
    "all_neg_samples = [sample for i, sample in enumerate(all_samples) if all_labels[i][0] == 1.]\n",
    "len(all_pos_samples), len(all_neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanceamento das classes (50% positivas e 50% negativas)\n",
    "min_samples = min(len(all_pos_samples), len(all_neg_samples))\n",
    "total_samples = 2*min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: 2669 (59.9910%)\n",
      "\t* positivas: 1334 (49.9813%)\n",
      "\t* negativas: 1335 (50.0187%)\n",
      "Validação: 890 (20.0045%)\n",
      "\t* positivas: 445 (50.0000%)\n",
      "\t* negativas: 445 (50.0000%)\n",
      "Teste: 890 (20.0045%)\n",
      "\t* positivas: 445 (50.0000%)\n",
      "\t* negativas: 445 (50.0000%)\n"
     ]
    }
   ],
   "source": [
    "# Obtém quantidade de amostras para treino, validação e teste\n",
    "# 60% treino \n",
    "qty_samples_train = round(0.6*total_samples)\n",
    "qty_samples_train_pos = round(qty_samples_train/2)\n",
    "qty_samples_train_neg = qty_samples_train - qty_samples_train_pos\n",
    "# 20% validação\n",
    "qty_samples_val = round(0.2*total_samples)\n",
    "qty_samples_val_pos = round(qty_samples_val/2)\n",
    "qty_samples_val_neg = qty_samples_val - qty_samples_val_pos\n",
    "# 20% teste\n",
    "qty_samples_test = round(0.2*total_samples)\n",
    "qty_samples_test_pos = round(qty_samples_test/2)\n",
    "qty_samples_test_neg = qty_samples_test - qty_samples_test_pos\n",
    "# Quantidade total de amostras\n",
    "total_samples = qty_samples_train + qty_samples_val + qty_samples_test\n",
    "# Treino\n",
    "print(f'Treino: {qty_samples_train} ({100*qty_samples_train/total_samples:.4f}%)')\n",
    "print(f'\\t* positivas: {qty_samples_train_pos} ({100*qty_samples_train_pos/qty_samples_train:.4f}%)')\n",
    "print(f'\\t* negativas: {qty_samples_train_neg} ({100*qty_samples_train_neg/qty_samples_train:.4f}%)')\n",
    "# Validação\n",
    "print(f'Validação: {qty_samples_val} ({100*qty_samples_val/total_samples:.4f}%)')\n",
    "print(f'\\t* positivas: {qty_samples_val_pos} ({100*qty_samples_val_pos/qty_samples_val:.4f}%)')\n",
    "print(f'\\t* negativas: {qty_samples_val_neg} ({100*qty_samples_val_neg/qty_samples_val:.4f}%)')\n",
    "# Teste\n",
    "print(f'Teste: {qty_samples_test} ({100*qty_samples_test/total_samples:.4f}%)')\n",
    "print(f'\\t* positivas: {qty_samples_test_pos} ({100*qty_samples_test_pos/qty_samples_test:.4f}%)')\n",
    "print(f'\\t* negativas: {qty_samples_test_neg} ({100*qty_samples_test_neg/qty_samples_test:.4f}%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo datasets de treino, validação e teste\n",
    "# Treino\n",
    "samples_train = all_pos_samples[0:qty_samples_train_pos] + \\\n",
    "                all_neg_samples[0:qty_samples_train_neg]\n",
    "labels_train = ([1]*qty_samples_train_pos) + \\\n",
    "               ([0]*qty_samples_train_neg)\n",
    "# Validação\n",
    "samples_validation = all_pos_samples[qty_samples_train_pos:qty_samples_train_pos+qty_samples_val_pos] + \\\n",
    "                     all_neg_samples[qty_samples_train_neg:qty_samples_train_neg+qty_samples_val_neg]\n",
    "labels_validation = ([1]*qty_samples_val_pos) + ([0]*qty_samples_val_neg)\n",
    "# Teste\n",
    "samples_test = all_pos_samples[qty_samples_train_pos+qty_samples_val_pos:qty_samples_train_pos+qty_samples_val_pos+qty_samples_test_pos] + \\\n",
    "               all_neg_samples[qty_samples_train_neg+qty_samples_val_neg:qty_samples_train_neg+qty_samples_val_neg+qty_samples_test_neg]\n",
    "labels_test = ([1]*qty_samples_test_pos) + ([0]*qty_samples_test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embaralhando amostras\n",
    "import random\n",
    "random.seed(123)\n",
    "# treino\n",
    "idxes = list(range(len(samples_train)))\n",
    "random.shuffle(idxes)\n",
    "samples_train = [samples_train[i] for i in idxes]\n",
    "labels_train = [labels_train[i] for i in idxes]\n",
    "# validação\n",
    "idxes = list(range(len(samples_validation)))\n",
    "random.shuffle(idxes)\n",
    "samples_validation = [samples_validation[i] for i in idxes]\n",
    "labels_validation = [labels_validation[i] for i in idxes]\n",
    "# teste\n",
    "idxes = list(range(len(samples_test)))\n",
    "random.shuffle(idxes)\n",
    "samples_test = [samples_test[i] for i in idxes]\n",
    "labels_test = [labels_test[i] for i in idxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos salvos em ../datasets\n"
     ]
    }
   ],
   "source": [
    "# Salvando os datasets de treino, teste e validação\n",
    "dir_datasets = '../datasets'\n",
    "if not isdir(dir_datasets):\n",
    "    mkdir(dir_datasets)\n",
    "pickle.dump({'samples': samples_train, 'labels': labels_train}, open(join(dir_datasets,'train_dataset.pickle'), 'wb'))\n",
    "pickle.dump({'samples': samples_validation, 'labels': labels_validation}, open(join(dir_datasets,'validation_dataset.pickle'), 'wb'))\n",
    "pickle.dump({'samples': samples_test, 'labels': labels_test}, open(join(dir_datasets,'test_dataset.pickle'), 'wb'))\n",
    "\n",
    "print(f'Arquivos salvos em {dir_datasets}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixforce",
   "language": "python",
   "name": "pixforce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
